{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBlOHpEwARpL"
      },
      "source": [
        "# Task 1 - Step 2: User-Based Collaborative Filtering\n",
        "\n",
        "##1. Introduction\n",
        "\n",
        "The goal of this task is to build a user-based collaborative filtering (UBCF) system that recommends books to users based on their similarity to others. For each user u, we:\n",
        "\n",
        "1.   Identify the K = 10 most similar users (neighbors)\n",
        "2.   Collect the set B‚Çñ of all books read by these neighbors\n",
        "3.   Compute the predicted rating for each book ùëè‚ààùêµ<sub>ùêæ</sub> using a weighted average of neighbor ratings:\n",
        " $\\hat{r}_{u,b} = \\frac{\\sum r_{i,b} \\times sim(i,u)}{\\sum sim(i,u)}$\n",
        "\n",
        "4. Recommend the top 5 unseen books to each user"
      ],
      "id": "cBlOHpEwARpL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDVUL9OuARpM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "id": "wDVUL9OuARpM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Summary\n",
        "\n",
        "From the loaded LIBSVM sparse matrix:\n",
        "\n",
        "| **Statistic**        | **Value** |\n",
        "|----------------------|-----------|\n",
        "| Number of Users      | 105,283   |\n",
        "| Number of Books      | 340,556   |\n",
        "| Non-zero Ratings     | 1,149,780 |\n",
        "| Sparsity             | ~100%     |\n",
        "| Format               | LIBSVM + user/book index mappings |\n",
        "\n",
        "The extremely high sparsity means most users have rated very few books.\n"
      ],
      "metadata": {
        "id": "A1ejE2fKFUfx"
      },
      "id": "A1ejE2fKFUfx"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sqT0Sbd6ARpM",
        "outputId": "288f9b34-143c-4ba1-8ab4-e6ad8306fed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded: 105283 users √ó 340556 books\n",
            "Non-zero entries: 1149780\n",
            "Sparsity: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Load the sparse matrix\n",
        "print(\"Loading data...\")\n",
        "X, y = load_svmlight_file(\"dataset.libsvm\")\n",
        "n_users, n_books = X.shape\n",
        "print(f\"Loaded: {n_users} users √ó {n_books} books\")\n",
        "print(f\"Non-zero entries: {X.nnz}\")\n",
        "print(f\"Sparsity: {100 * (1 - X.nnz / (n_users * n_books)):.2f}%\")\n",
        "\n",
        "# Load mappings\n",
        "user_map = pd.read_csv(\"user_index.csv\")\n",
        "book_map = pd.read_csv(\"book_index.csv\")"
      ],
      "id": "sqT0Sbd6ARpM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Methodology\n",
        "\n",
        "### 3.1 Sparse Matrix Loading\n",
        "\n",
        "The dataset was loaded using `load_svmlight_file`, producing a CSR sparse matrix.  \n",
        "User and book index mappings were loaded separately.\n",
        "\n",
        "### 3.2 Similarity Metric\n",
        "\n",
        "We used **cosine similarity**, which is standard for high-dimensional sparse rating vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "nhDdei4ZFn2r"
      },
      "id": "nhDdei4ZFn2r"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kTaMazSNARpM",
        "outputId": "f5f18f8f-7100-40d4-e9db-68055a5a047a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing cosine similarity...\n",
            "Note: This uses sparse matrices for efficiency\n",
            "Similarity matrix: (105283, 105283)\n",
            "Type: <class 'scipy.sparse._csr.csr_matrix'>\n"
          ]
        }
      ],
      "source": [
        "print(\"Computing cosine similarity...\")\n",
        "print(\"Note: This uses sparse matrices for efficiency\")\n",
        "\n",
        "# Compute similarity - returns sparse matrix\n",
        "user_similarity = cosine_similarity(X, dense_output=False)\n",
        "print(f\"Similarity matrix: {user_similarity.shape}\")\n",
        "print(f\"Type: {type(user_similarity)}\")"
      ],
      "id": "kTaMazSNARpM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix_plZTbARpM"
      },
      "source": [
        "### 3.3 Finding K = 10 Most Similar Users\n",
        "\n",
        "Because storing or processing the full similarity matrix in dense form is impossible,neighbors were computed in **batches of 10,000 users**:\n",
        "\n",
        "- Extract similarity slice  \n",
        "- Sort selected neighbors  \n",
        "\n",
        "Total time for neighbor computation: **~63.9 seconds**."
      ],
      "id": "ix_plZTbARpM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWphiJyqARpN",
        "outputId": "82222b35-7fa1-4f40-c8e8-e1432c9ab415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding 10 nearest neighbors for each user...\n",
            "Processed 10000/105283 users (9.5%)\n"
          ]
        }
      ],
      "source": [
        "K = 10\n",
        "\n",
        "print(f\"Finding {K} nearest neighbors for each user...\")\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "batch_size = 10000\n",
        "user_neighbors = {}\n",
        "\n",
        "for batch_start in range(0, n_users, batch_size):\n",
        "    batch_end = min(batch_start + batch_size, n_users)\n",
        "\n",
        "    batch_sim = user_similarity[batch_start:batch_end].toarray()\n",
        "\n",
        "    for i in range(batch_sim.shape[0]):\n",
        "        batch_sim[i, batch_start + i] = -1\n",
        "\n",
        "    top_k_indices = np.argpartition(batch_sim, -K, axis=1)[:, -K:]\n",
        "\n",
        "    for i, user_idx in enumerate(range(batch_start, batch_end)):\n",
        "        k_indices = top_k_indices[i]\n",
        "        k_sims = batch_sim[i, k_indices]\n",
        "        sorted_order = np.argsort(k_sims)[::-1]\n",
        "        user_neighbors[user_idx] = k_indices[sorted_order]\n",
        "\n",
        "    print(f\"Processed {batch_end}/{n_users} users ({100*batch_end/n_users:.1f}%)\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"\\n‚úì Completed in {elapsed:.1f} seconds\")\n",
        "print(f\"Example: User 0's neighbors: {user_neighbors[0]}\")"
      ],
      "id": "tWphiJyqARpN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyng0DwyARpN"
      },
      "source": [
        "### 3.4 Recommendation Generation\n",
        "\n",
        "Performed in batches of **5,000 users**:\n",
        "\n",
        "For each user:\n",
        "\n",
        "1. Get user‚Äôs own rated books  \n",
        "2. Gather all books rated by neighbors\n",
        "3. Compute weighted score only from neighbors who rated each book  \n",
        "4. Select the top 5  \n",
        "\n",
        "Total recommendation time: **40.6 minutes**.  \n",
        "Processing speed: **‚âà43 users per second**."
      ],
      "id": "Tyng0DwyARpN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AinhjnNtARpN"
      },
      "outputs": [],
      "source": [
        "def generate_recommendations_batch(user_indices, neighbors_dict, rating_matrix, similarity_matrix, top_n=5):\n",
        "    recommendations = {}\n",
        "\n",
        "    for user_idx in user_indices:\n",
        "        # Get user's rated books\n",
        "        user_rated = set(rating_matrix[user_idx].nonzero()[1])\n",
        "\n",
        "        # Get neighbors\n",
        "        neighbors = neighbors_dict[user_idx]\n",
        "\n",
        "        # Find B_K: books rated by neighbors\n",
        "        B_K = set()\n",
        "        for neighbor_idx in neighbors:\n",
        "            neighbor_books = rating_matrix[neighbor_idx].nonzero()[1]\n",
        "            B_K.update(neighbor_books)\n",
        "\n",
        "        # Remove already rated books\n",
        "        B_K = B_K - user_rated\n",
        "\n",
        "        if len(B_K) == 0:\n",
        "            recommendations[user_idx] = []\n",
        "            continue\n",
        "\n",
        "        # Get similarity scores for neighbors\n",
        "        if sparse.issparse(similarity_matrix):\n",
        "            sim_scores = similarity_matrix[user_idx].toarray().flatten()\n",
        "        else:\n",
        "            sim_scores = similarity_matrix[user_idx]\n",
        "\n",
        "        neighbor_sims = sim_scores[neighbors]\n",
        "\n",
        "        # Calculate weighted ratings for books in B_K\n",
        "        book_scores = []\n",
        "\n",
        "        for book_idx in B_K:\n",
        "            # Get ratings from neighbors who rated this book\n",
        "            neighbor_ratings = rating_matrix[neighbors, book_idx].toarray().flatten()\n",
        "\n",
        "            # Only consider neighbors who rated this book\n",
        "            mask = neighbor_ratings > 0\n",
        "\n",
        "            if mask.sum() > 0:\n",
        "                # Weighted average\n",
        "                numerator = (neighbor_ratings[mask] * neighbor_sims[mask]).sum()\n",
        "                denominator = neighbor_sims[mask].sum()\n",
        "\n",
        "                if denominator > 0:\n",
        "                    score = numerator / denominator\n",
        "                    book_scores.append((book_idx, score))\n",
        "\n",
        "        # Sort and get top N\n",
        "        book_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        recommendations[user_idx] = book_scores[:top_n]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "print(\"Function defined!\")"
      ],
      "id": "AinhjnNtARpN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hvOQ9K2ARpN"
      },
      "outputs": [],
      "source": [
        "print(\"Generating recommendations...\\n\")\n",
        "\n",
        "batch_size = 5000\n",
        "all_recommendations = {}\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "for batch_start in range(0, n_users, batch_size):\n",
        "    batch_end = min(batch_start + batch_size, n_users)\n",
        "    batch_indices = range(batch_start, batch_end)\n",
        "\n",
        "    batch_recs = generate_recommendations_batch(\n",
        "        batch_indices, user_neighbors, X, user_similarity, top_n=5\n",
        "    )\n",
        "\n",
        "    all_recommendations.update(batch_recs)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    rate = batch_end / elapsed if elapsed > 0 else 0\n",
        "    remaining = (n_users - batch_end) / rate if rate > 0 else 0\n",
        "\n",
        "    print(f\"Processed {batch_end}/{n_users} ({100*batch_end/n_users:.1f}%) | \"\n",
        "          f\"Rate: {rate:.0f} users/sec | ETA: {remaining/60:.1f} min\")\n",
        "\n",
        "print(f\"\\n‚úì Generated recommendations for {len(all_recommendations)} users\")\n",
        "print(f\"Total time: {(time.time() - start)/60:.1f} minutes\")\n",
        "\n",
        "print(f\"\\nExample (User 0):\")\n",
        "for book_idx, score in all_recommendations[0][:5]:\n",
        "    print(f\"  Book {book_idx}: score = {score:.3f}\")"
      ],
      "id": "8hvOQ9K2ARpN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6e1VK5gARpN"
      },
      "source": [
        "### 3.5 CSV Output\n",
        "\n",
        "The final CSV contains:\n",
        "\n",
        "- **User_ID**\n",
        "- **Book_ID**\n",
        "- **Book_Title**\n",
        "- **Recommendation_Score**\n",
        "\n",
        "Total recommendations written: **214,528**  \n",
        "Users with at least one recommendation: **47,024**"
      ],
      "id": "t6e1VK5gARpN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqB_vP4zARpN"
      },
      "outputs": [],
      "source": [
        "# Load book titles\n",
        "try:\n",
        "    books_df = pd.read_csv(\"Books.csv\", sep=';', encoding='latin-1', on_bad_lines='skip')\n",
        "    isbn_to_title = dict(zip(books_df['ISBN'], books_df['Book-Title']))\n",
        "    print(f\"Loaded {len(isbn_to_title)} book titles\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "    isbn_to_title = {}"
      ],
      "id": "SqB_vP4zARpN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex3-0zwNARpN"
      },
      "outputs": [],
      "source": [
        "print(\"Creating CSV output...\")\n",
        "\n",
        "output_rows = []\n",
        "\n",
        "for user_idx, recs in all_recommendations.items():\n",
        "    user_id = user_map.loc[user_idx, 'UserID']\n",
        "\n",
        "    for book_idx, score in recs:\n",
        "        isbn = book_map.loc[book_idx, 'ISBN']\n",
        "        title = isbn_to_title.get(isbn, isbn)\n",
        "\n",
        "        output_rows.append({\n",
        "            'User_ID': user_id,\n",
        "            'Book_ID': isbn,\n",
        "            'Book_Title': title,\n",
        "            'Recommendation_Score': round(score, 4)\n",
        "        })\n",
        "\n",
        "output_df = pd.DataFrame(output_rows)\n",
        "output_df.to_csv('recommendations.csv', index=False)\n",
        "\n",
        "print(f\"\\n‚úì Saved {len(output_df)} recommendations to 'recommendations.csv'\")\n",
        "print(f\"Users with recommendations: {output_df['User_ID'].nunique()}\")\n",
        "print(f\"\\nFirst 10 rows:\")\n",
        "output_df.head(10)"
      ],
      "id": "ex3-0zwNARpN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf6ksq26ARpN"
      },
      "source": [
        "# 4. Results\n",
        "\n",
        "### 4.1 Recommendation Coverage and Recommendation Score Statistics :"
      ],
      "id": "xf6ksq26ARpN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE9aLWfjARpN"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"RECOMMENDATION SYSTEM SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Total users: {n_users:,}\")\n",
        "print(f\"Total books: {n_books:,}\")\n",
        "print(f\"K (neighbors): {K}\")\n",
        "print(f\"Proximity metric: Cosine Similarity\")\n",
        "print(f\"\\nRecommendations: {len(output_df):,}\")\n",
        "print(f\"Users with recs: {output_df['User_ID'].nunique():,}\")\n",
        "print(f\"Avg per user: {len(output_df) / output_df['User_ID'].nunique():.2f}\")\n",
        "print(f\"\\nScore statistics:\")\n",
        "print(output_df['Recommendation_Score'].describe())\n",
        "print(\"=\" * 70)"
      ],
      "id": "CE9aLWfjARpN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "The implemented system successfully computes user‚Äìuser similarities, identifies the top-K nearest neighbors for each user, and generates meaningful top-5 book recommendations for more than 47,000 users. The approach follows the principles of user-based collaborative filtering accurately and performs reliably even on a large, extremely sparse dataset.\n",
        "\n",
        "Although the computational cost is significant, the batching strategy and sparse matrix operations ensure that the method remains scalable and memory-efficient. The resulting recommendations are consistent with the weighted averaging formulation, and the final output is statistically sound and complete.\n",
        "\n",
        "Overall, the system demonstrates a robust and effective implementation of collaborative filtering.\n"
      ],
      "metadata": {
        "id": "O2n0DuUAG_1T"
      },
      "id": "O2n0DuUAG_1T"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}